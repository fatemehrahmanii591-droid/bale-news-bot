import requests
from bs4 import BeautifulSoup
from datetime import datetime
import pytz

class NewsAggregator:
    def __init__(self):
        self.keywords = [
            'Ø´Ø±Ú©Øª Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ùˆ ØªÙˆØ³Ø¹Ù‡ Ú¯Ø§Ø² Ø§ÛŒØ±Ø§Ù†',
            'Ø´Ø±Ú©Øª Ù…Ù„ÛŒ Ú¯Ø§Ø² Ø§ÛŒØ±Ø§Ù†',
            'Ù…Ù‡Ù†Ø¯Ø³ Ø¨Ù‡Ù†Ø§Ù… Ù…ÛŒØ±Ø²Ø§ÛŒÛŒ',
            'Ù…ÛŒØ±Ø²Ø§ÛŒÛŒ',
            'Ø®Ø· Ù„ÙˆÙ„Ù‡ Ú¯Ø§Ø²',
            'Ø§ÛŒØ³ØªÚ¯Ø§Ù‡ ØªÙ‚ÙˆÛŒØª ÙØ´Ø§Ø± Ú¯Ø§Ø²',
            'ØµÙ†Ø¹Øª Ú¯Ø§Ø² Ø§ÛŒØ±Ø§Ù†',
            'Ú¯Ø§Ø² Ø·Ø¨ÛŒØ¹ÛŒ Ø§ÛŒØ±Ø§Ù†'
        ]
        
        self.sources = {
            'Ù…Ù‡Ø±': 'https://www.mehrnews.com',
            'Ø§ÛŒØ³Ù†Ø§': 'https://www.isna.ir',
            'Ø§ÛŒØ±Ù†Ø§': 'https://www.irna.ir',
            'Ø´Ø§Ù†Ø§': 'https://www.shana.ir',
        }
    
    def search_mehr_news(self):
        """Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ Ù…Ù‡Ø±"""
        news_list = []
        try:
            for keyword in self.keywords[:3]:  # Ø³Ù‡ Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ Ø§ØµÙ„ÛŒ
                url = f"https://www.mehrnews.com/search?text={keyword}"
                response = requests.get(url, timeout=10)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    articles = soup.find_all('div', class_='item', limit=5)
                    
                    for article in articles:
                        try:
                            title_elem = article.find('a')
                            if title_elem:
                                title = title_elem.get_text(strip=True)
                                link = 'https://www.mehrnews.com' + title_elem.get('href')
                                
                                # Ú†Ú© Ú©Ø±Ø¯Ù† Ù…Ø±ØªØ¨Ø· Ø¨ÙˆØ¯Ù†
                                if self._is_relevant(title):
                                    news_list.append({
                                        'title': title,
                                        'link': link,
                                        'source': 'Ù…Ù‡Ø±',
                                        'date': datetime.now(pytz.timezone('Asia/Tehran')).strftime('%Y-%m-%d')
                                    })
                        except:
                            continue
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ù‡Ø±: {e}")
        
        return news_list
    
    def search_isna_news(self):
        """Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ Ø§ÛŒØ³Ù†Ø§"""
        news_list = []
        try:
            for keyword in self.keywords[:3]:
                url = f"https://www.isna.ir/search?search={keyword}"
                response = requests.get(url, timeout=10)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    articles = soup.find_all('div', class_='news-img-desc', limit=5)
                    
                    for article in articles:
                        try:
                            title_elem = article.find('a')
                            if title_elem:
                                title = title_elem.get('title', '')
                                link = 'https://www.isna.ir' + title_elem.get('href')
                                
                                if self._is_relevant(title):
                                    news_list.append({
                                        'title': title,
                                        'link': link,
                                        'source': 'Ø§ÛŒØ³Ù†Ø§',
                                        'date': datetime.now(pytz.timezone('Asia/Tehran')).strftime('%Y-%m-%d')
                                    })
                        except:
                            continue
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§ÛŒØ³Ù†Ø§: {e}")
        
        return news_list
    
    def _is_relevant(self, text):
        """Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨ÙˆØ¯Ù† Ø¹Ù†ÙˆØ§Ù†"""
        text_lower = text.lower()
        relevant_words = ['Ú¯Ø§Ø²', 'Ù…ÛŒØ±Ø²Ø§ÛŒÛŒ', 'Ø®Ø· Ù„ÙˆÙ„Ù‡', 'Ø§ÛŒØ³ØªÚ¯Ø§Ù‡', 'Ø´Ø±Ú©Øª Ù…Ù„ÛŒ']
        return any(word in text_lower for word in relevant_words)
    
    def get_all_news(self):
        """Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø² ØªÙ…Ø§Ù… Ù…Ù†Ø§Ø¨Ø¹"""
        print("ğŸ” Ø´Ø±ÙˆØ¹ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù…Ù†Ø§Ø¨Ø¹...")
        all_news = []
        
        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù…Ù‡Ø±
        mehr_news = self.search_mehr_news()
        print(f"âœ… Ù…Ù‡Ø±: {len(mehr_news)} Ø®Ø¨Ø±")
        all_news.extend(mehr_news)
        
        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø§ÛŒØ³Ù†Ø§
        isna_news = self.search_isna_news()
        print(f"âœ… Ø§ÛŒØ³Ù†Ø§: {len(isna_news)} Ø®Ø¨Ø±")
        all_news.extend(isna_news)
        
        # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§
        unique_news = []
        seen_links = set()
        for news in all_news:
            if news['link'] not in seen_links:
                seen_links.add(news['link'])
                unique_news.append(news)
        
        print(f"ğŸ“Š Ù…Ø¬Ù…ÙˆØ¹ Ø§Ø®Ø¨Ø§Ø± ÛŒÚ©ØªØ§: {len(unique_news)}")
        return unique_news
